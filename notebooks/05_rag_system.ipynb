{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Amazon Sales Dataset - RAG System**\n",
        "\n",
        "**Components**\n",
        "- Dense Retrieval: FAISS\n",
        "- Sparse Retrieval: TF-IDF\n",
        "- Reranking: CrossEncoder\n",
        "- Generation: Gemini API\n",
        "\n",
        "**Model Storage**\n",
        "Cached in `../models/rag/` for fast production loading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q sentence-transformers faiss-cpu google-generativeai scikit-learn pandas numpy tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import faiss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import google.generativeai as genai\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config loaded. Model dir: ../models/rag\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class RAGConfig:\n",
        "    # Paths\n",
        "    data_path: str = \"../data/processed/amazon.csv\"\n",
        "    model_dir: str = \"../models/rag\"\n",
        "    \n",
        "    # Models\n",
        "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
        "    gemini_model: str = \"models/gemini-2.5-flash\"\n",
        "    \n",
        "    # Parameters\n",
        "    embedding_dim: int = 384\n",
        "    top_k_dense: int = 20\n",
        "    top_k_sparse: int = 20\n",
        "    top_k_final: int = 10\n",
        "    hybrid_alpha: float = 0.6\n",
        "    \n",
        "    # Reranking\n",
        "    use_reranker: bool = True\n",
        "    rerank_top_k: int = 5\n",
        "    \n",
        "    # LLM\n",
        "    gemini_temperature: float = 0.7\n",
        "    gemini_max_tokens: int = 12000\n",
        "    \n",
        "    # Cache\n",
        "    cache_embeddings: bool = True\n",
        "    cache_tfidf: bool = True\n",
        "    cache_index: bool = True\n",
        "\n",
        "config = RAGConfig()\n",
        "Path(config.model_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Config loaded. Model dir: {config.model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini client configured\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "def setup_gemini(api_key: Optional[str] = None):\n",
        "    \"\"\"Configure Gemini API with new client-based approach.\"\"\"\n",
        "    if api_key is None:\n",
        "        api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "    \n",
        "    if not api_key:\n",
        "        print(\"WARNING: GEMINI_API_KEY not found\")\n",
        "        print(\"Set with: os.environ['GEMINI_API_KEY'] = 'your-key'\")\n",
        "        return None\n",
        "    \n",
        "    # New API: Create client instead of configure\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    print(f\"Gemini client configured\")\n",
        "    return client\n",
        "\n",
        "os.environ['GEMINI_API_KEY'] = 'AIzaSyDcIoUJHsLeOpAeKYiYFTEFjtUGLxZdmEQ'\n",
        "gemini_client = setup_gemini()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_chat_session():\n",
        "    \"\"\"Create chat session with new API.\"\"\"\n",
        "    if not gemini_client:\n",
        "        return None\n",
        "    \n",
        "    # New chat API\n",
        "    chat = gemini_client.chats.create(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.7,\n",
        "            max_output_tokens=32000\n",
        "        )\n",
        "    )\n",
        "    return chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from ../data/processed/amazon.csv\n",
            "Loaded 1351 products\n",
            "Preprocessing complete\n"
          ]
        }
      ],
      "source": [
        "def load_and_preprocess_data(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load and preprocess product dataset.\"\"\"\n",
        "    print(f\"Loading data from {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"Loaded {len(df)} products\")\n",
        "    \n",
        "    df['rating'] = df['rating'].fillna(0)\n",
        "    df['rating_count'] = df['rating_count'].fillna(0)\n",
        "    df['about_product'] = df['about_product'].fillna('')\n",
        "    df['category'] = df['category'].fillna('Unknown')\n",
        "    \n",
        "    def create_context(row):\n",
        "        parts = []\n",
        "        if pd.notna(row.get('product_name')):\n",
        "            parts.append(f\"Product: {row['product_name']}\")\n",
        "        if pd.notna(row.get('category')):\n",
        "            parts.append(f\"Category: {row['category']}\")\n",
        "        if pd.notna(row.get('about_product')) and str(row['about_product']) != '':\n",
        "            parts.append(f\"Description: {str(row['about_product'])[:300]}\")\n",
        "        if pd.notna(row.get('discounted_price')):\n",
        "            parts.append(f\"Price: {row['discounted_price']}\")\n",
        "        if pd.notna(row.get('rating')):\n",
        "            parts.append(f\"Rating: {row['rating']}/5\")\n",
        "        if pd.notna(row.get('rating_count')):\n",
        "            parts.append(f\"Reviews: {int(row['rating_count'])}\")\n",
        "        return \" | \".join(parts)\n",
        "    \n",
        "    df['product_context'] = df.apply(create_context, axis=1)\n",
        "    print(\"Preprocessing complete\")\n",
        "    return df\n",
        "\n",
        "df = load_and_preprocess_data(config.data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Embedding Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embeddings from ../models/rag\\product_embeddings.npy\n",
            "Loaded: (1351, 384)\n"
          ]
        }
      ],
      "source": [
        "def generate_or_load_embeddings(texts: List[str], model_name: str, cache_path: str = None):\n",
        "    \"\"\"Generate embeddings or load from cache.\"\"\"\n",
        "    if cache_path and os.path.exists(cache_path):\n",
        "        print(f\"Loading embeddings from {cache_path}\")\n",
        "        embeddings = np.load(cache_path)\n",
        "        print(f\"Loaded: {embeddings.shape}\")\n",
        "    else:\n",
        "        print(f\"Generating embeddings: {model_name}\")\n",
        "        model = SentenceTransformer(model_name)\n",
        "        embeddings = model.encode(\n",
        "            texts, batch_size=32, show_progress_bar=True,\n",
        "            convert_to_numpy=True, normalize_embeddings=True\n",
        "        )\n",
        "        print(f\"Generated: {embeddings.shape}\")\n",
        "        if cache_path:\n",
        "            np.save(cache_path, embeddings)\n",
        "            print(f\"Cached to {cache_path}\")\n",
        "    \n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model, embeddings\n",
        "\n",
        "cache_path = os.path.join(config.model_dir, \"product_embeddings.npy\")\n",
        "embedding_model, product_embeddings = generate_or_load_embeddings(\n",
        "    df['product_context'].tolist(),\n",
        "    config.embedding_model,\n",
        "    cache_path=cache_path if config.cache_embeddings else None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. FAISS Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading FAISS from ../models/rag\\faiss_index.bin\n",
            "Loaded: 1351 vectors\n"
          ]
        }
      ],
      "source": [
        "def build_or_load_faiss(embeddings: np.ndarray, cache_path: str = None):\n",
        "    \"\"\"Build FAISS index or load from cache.\"\"\"\n",
        "    if cache_path and os.path.exists(cache_path):\n",
        "        print(f\"Loading FAISS from {cache_path}\")\n",
        "        index = faiss.read_index(cache_path)\n",
        "        print(f\"Loaded: {index.ntotal} vectors\")\n",
        "    else:\n",
        "        embeddings = embeddings.astype(np.float32)\n",
        "        dim = embeddings.shape[1]\n",
        "        print(f\"Building FAISS (dim={dim})\")\n",
        "        index = faiss.IndexFlatIP(dim)\n",
        "        index.add(embeddings)\n",
        "        print(f\"Built: {index.ntotal} vectors\")\n",
        "        if cache_path:\n",
        "            faiss.write_index(index, cache_path)\n",
        "            print(f\"Cached to {cache_path}\")\n",
        "    return index\n",
        "\n",
        "cache_path = os.path.join(config.model_dir, \"faiss_index.bin\")\n",
        "faiss_index = build_or_load_faiss(\n",
        "    product_embeddings,\n",
        "    cache_path=cache_path if config.cache_index else None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. TF-IDF Sparse Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TF-IDF from cache\n",
            "Loaded: (1351, 5000)\n"
          ]
        }
      ],
      "source": [
        "def build_or_load_tfidf(documents: List[str], cache_dir: str = None):\n",
        "    \"\"\"Build TF-IDF or load from cache.\"\"\"\n",
        "    vec_path = os.path.join(cache_dir, \"tfidf_vectorizer.pkl\") if cache_dir else None\n",
        "    mat_path = os.path.join(cache_dir, \"tfidf_matrix.pkl\") if cache_dir else None\n",
        "    \n",
        "    if vec_path and os.path.exists(vec_path) and os.path.exists(mat_path):\n",
        "        print(\"Loading TF-IDF from cache\")\n",
        "        with open(vec_path, \"rb\") as f:\n",
        "            vectorizer = pickle.load(f)\n",
        "        with open(mat_path, \"rb\") as f:\n",
        "            doc_vectors = pickle.load(f)\n",
        "        print(f\"Loaded: {doc_vectors.shape}\")\n",
        "    else:\n",
        "        print(\"Building TF-IDF\")\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=5000, max_df=0.8, min_df=2,\n",
        "            ngram_range=(1, 2), lowercase=True\n",
        "        )\n",
        "        doc_vectors = vectorizer.fit_transform(documents)\n",
        "        print(f\"Built: {doc_vectors.shape}\")\n",
        "        if vec_path:\n",
        "            with open(vec_path, \"wb\") as f:\n",
        "                pickle.dump(vectorizer, f)\n",
        "            with open(mat_path, \"wb\") as f:\n",
        "                pickle.dump(doc_vectors, f)\n",
        "            print(\"Cached\")\n",
        "    return vectorizer, doc_vectors\n",
        "\n",
        "tfidf_vectorizer, tfidf_vectors = build_or_load_tfidf(\n",
        "    df['product_context'].tolist(),\n",
        "    cache_dir=config.model_dir if config.cache_tfidf else None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Reranker Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
            "Loaded\n"
          ]
        }
      ],
      "source": [
        "def load_reranker(model_name: str):\n",
        "    \"\"\"Load CrossEncoder reranker.\"\"\"\n",
        "    print(f\"Loading reranker: {model_name}\")\n",
        "    reranker = CrossEncoder(model_name)\n",
        "    print(\"Loaded\")\n",
        "    return reranker\n",
        "\n",
        "reranker_model = load_reranker(config.reranker_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Hybrid Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hybrid retrieval ready\n"
          ]
        }
      ],
      "source": [
        "def hybrid_retrieve(query: str, top_k: int = 10, use_reranker: bool = True) -> List[Dict]:\n",
        "    \"\"\"Hybrid retrieval: dense + sparse + reranking.\"\"\"\n",
        "    # Dense\n",
        "    q_emb = embedding_model.encode(\n",
        "        query, convert_to_numpy=True, normalize_embeddings=True\n",
        "    ).astype(np.float32).reshape(1, -1)\n",
        "    d_scores, d_idx = faiss_index.search(q_emb, k=config.top_k_dense)\n",
        "    d_scores, d_idx = d_scores[0], d_idx[0]\n",
        "    \n",
        "    # Sparse\n",
        "    q_vec = tfidf_vectorizer.transform([query])\n",
        "    s_scores = cosine_similarity(q_vec, tfidf_vectors)[0]\n",
        "    s_idx = np.argsort(s_scores)[::-1][:config.top_k_sparse]\n",
        "    s_scores = s_scores[s_idx]\n",
        "    \n",
        "    # Combine\n",
        "    combined = {}\n",
        "    for idx, score in zip(d_idx, d_scores):\n",
        "        combined[int(idx)] = config.hybrid_alpha * float(score)\n",
        "    for idx, score in zip(s_idx, s_scores):\n",
        "        idx = int(idx)\n",
        "        combined[idx] = combined.get(idx, 0.0) + (1 - config.hybrid_alpha) * float(score)\n",
        "    \n",
        "    sorted_items = sorted(combined.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_idx = [i for i, _ in sorted_items[:config.top_k_final]]\n",
        "    top_scores = [s for _, s in sorted_items[:config.top_k_final]]\n",
        "    \n",
        "    # Rerank\n",
        "    if use_reranker and reranker_model:\n",
        "        contexts = [df.iloc[i]['product_context'] for i in top_idx]\n",
        "        pairs = [[query, c] for c in contexts]\n",
        "        r_scores = reranker_model.predict(pairs)\n",
        "        r_idx = np.argsort(r_scores)[::-1][:config.rerank_top_k]\n",
        "        final_idx = [top_idx[i] for i in r_idx]\n",
        "        final_scores = [r_scores[i] for i in r_idx]\n",
        "    else:\n",
        "        final_idx = top_idx[:top_k]\n",
        "        final_scores = top_scores[:top_k]\n",
        "    \n",
        "    # Format\n",
        "    results = []\n",
        "    for idx, score in zip(final_idx, final_scores):\n",
        "        p = df.iloc[idx]\n",
        "        results.append({\n",
        "            'index': int(idx),\n",
        "            'score': float(score),\n",
        "            'product_name': str(p.get('product_name', 'N/A')),\n",
        "            'category': str(p.get('category', 'N/A')),\n",
        "            'price': str(p.get('discounted_price', 'N/A')),\n",
        "            'rating': float(p.get('rating', 0)),\n",
        "            'rating_count': int(p.get('rating_count', 0)),\n",
        "            'description': str(p.get('about_product', ''))[:200],\n",
        "            'product_link': str(p.get('product_link', 'N/A'))\n",
        "        })\n",
        "    return results\n",
        "\n",
        "print(\"Hybrid retrieval ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Recommendation Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommendation generator ready\n"
          ]
        }
      ],
      "source": [
        "def generate_recommendation(query: str, retrieved: List[Dict]) -> str:\n",
        "    \"\"\"Generate AI recommendation with new Gemini API.\"\"\"\n",
        "    if not gemini_client:\n",
        "        return \"Gemini not configured\"\n",
        "    \n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"{i}. {p['product_name']}\\n\"\n",
        "        f\"   Category: {p['category']}\\n\"\n",
        "        f\"   Price: {p['price']}\\n\"\n",
        "        f\"   Rating: {p['rating']}/5 ({p['rating_count']} reviews)\\n\"\n",
        "        f\"   Description: {p['description']}\"\n",
        "        for i, p in enumerate(retrieved[:5], 1)\n",
        "    ])\n",
        "    \n",
        "    prompt = f\"\"\"Provide product recommendation based on query.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Products:\n",
        "{context}\n",
        "\n",
        "Please provide:\n",
        "1. Brief summary\n",
        "2. Top 2-3 recommendations with reasons\n",
        "3. Key features\n",
        "\n",
        "Keep response concise and helpful. Bên cạnh đó sử dụng Tiếng Việt trong phần trả lời.\"\"\"  \n",
        "    \n",
        "    try:\n",
        "        # NEW API: Use client.models.generate_content()\n",
        "        response = gemini_client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=config.gemini_temperature,\n",
        "                max_output_tokens=config.gemini_max_tokens,\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # Check if response has content\n",
        "        if not response.text:\n",
        "            return \"Response blocked or empty. Try rephrasing query.\"\n",
        "        \n",
        "        return response.text\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "print(\"Recommendation generator ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q&A ready\n"
          ]
        }
      ],
      "source": [
        "def answer_question(question: str) -> Dict:\n",
        "    \"\"\"Answer questions using RAG with new Gemini API.\"\"\"\n",
        "    if not gemini_client:\n",
        "        return {\"error\": \"Gemini not configured\", \"question\": question}\n",
        "    \n",
        "    try:\n",
        "        retrieved = hybrid_retrieve(question, top_k=5)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Retrieval error: {str(e)}\", \"question\": question}\n",
        "    \n",
        "    context = \"\\n\".join([\n",
        "        f\"- {p['product_name']} ({p['category']}): \"\n",
        "        f\"Price {p['price']}, Rating {p['rating']}/5. {p['description']}\"\n",
        "        for p in retrieved\n",
        "    ])\n",
        "    \n",
        "    prompt = f\"\"\"Answer this question about products.\n",
        "\n",
        "Question: \"{question}\"\n",
        "\n",
        "Available Products:\n",
        "{context}\n",
        "\n",
        "Provide a clear and helpful answer.Bên cạnh đó sử dụng Tiếng Việt trong phần trả lời.\"\"\"  \n",
        "    \n",
        "    try:\n",
        "        # NEW API\n",
        "        response = gemini_client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=config.gemini_temperature,\n",
        "                max_output_tokens=config.gemini_max_tokens,\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        if not response.text:\n",
        "            return {\n",
        "                \"error\": \"Response blocked or empty\",\n",
        "                \"question\": question,\n",
        "                \"products\": retrieved\n",
        "            }\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": response.text,\n",
        "            \"num_retrieved\": len(retrieved),\n",
        "            \"products\": retrieved\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e), \"question\": question}\n",
        "\n",
        "print(\"Q&A ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chatbot ready\n"
          ]
        }
      ],
      "source": [
        "class RAGChatBot:\n",
        "    \"\"\"Interactive chatbot.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "    \n",
        "    def chat(self, user_input: str, mode: str = \"recommend\") -> Dict:\n",
        "        \"\"\"Process input and generate response.\"\"\"\n",
        "        print(f\"\\nQuery: {user_input}\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        print(\"Retrieving...\")\n",
        "        start = time.time()\n",
        "        retrieved = hybrid_retrieve(user_input, top_k=5)\n",
        "        ret_time = time.time() - start\n",
        "        # print(f\"Retrieved {len(retrieved)} in {ret_time:.2f}s\\n\")\n",
        "        \n",
        "        # print(\"Products:\")\n",
        "        # print(\"-\"*80)\n",
        "        # for i, p in enumerate(retrieved, 1):\n",
        "        #     print(f\"{i}. {p['product_name']}\")\n",
        "        #     print(f\"   {p['category']} | {p['price']} | {p['rating']}/5\")\n",
        "        #     print(f\"   Score: {p['score']:.4f}\\n\")\n",
        "        \n",
        "        if mode == \"recommend\":\n",
        "            print(\"Generating...\\n\")\n",
        "            start = time.time()\n",
        "            rec = generate_recommendation(user_input, retrieved)\n",
        "            gen_time = time.time() - start\n",
        "            \n",
        "            # print(\"Recommendation:\")\n",
        "            # print(\"-\"*80)\n",
        "            # print(rec)\n",
        "            # print(\"-\"*80)\n",
        "            # print(f\"\\nGeneration: {gen_time:.2f}s\")\n",
        "            \n",
        "            result = {\n",
        "                \"mode\": \"recommend\",\n",
        "                \"query\": user_input,\n",
        "                \"products\": retrieved,\n",
        "                \"recommendation\": rec,\n",
        "                \"retrieval_time\": ret_time,\n",
        "                \"generation_time\": gen_time\n",
        "            }\n",
        "        else:\n",
        "            print(\"Answering...\\n\")\n",
        "            start = time.time()\n",
        "            ans = answer_question(user_input)\n",
        "            gen_time = time.time() - start\n",
        "            \n",
        "            if \"error\" in ans:\n",
        "                # print(f\"Error: {ans['error']}\")\n",
        "                result = ans\n",
        "            else:\n",
        "                # print(\"Answer:\")\n",
        "                # print(\"-\"*80)\n",
        "                # print(ans['answer'])\n",
        "                # print(\"-\"*80)\n",
        "                # print(f\"\\nGeneration: {gen_time:.2f}s\")\n",
        "                \n",
        "                result = {\n",
        "                    \"mode\": \"qa\",\n",
        "                    \"query\": user_input,\n",
        "                    \"answer\": ans['answer'],\n",
        "                    \"products\": retrieved,\n",
        "                    \"retrieval_time\": ret_time,\n",
        "                    \"generation_time\": gen_time\n",
        "                }\n",
        "        \n",
        "        self.history.append(result)\n",
        "        return result\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get statistics safely.\"\"\"\n",
        "        if not self.history:\n",
        "            return {\"queries\": 0, \"avg_retrieval\": 0, \"avg_generation\": 0}\n",
        "        \n",
        "        retrieval_times = [h.get('retrieval_time', 0) for h in self.history]\n",
        "        generation_times = [h.get('generation_time', 0) for h in self.history]\n",
        "        \n",
        "        return {\n",
        "            \"queries\": len(self.history),\n",
        "            \"avg_retrieval\": np.mean(retrieval_times),\n",
        "            \"avg_generation\": np.mean(generation_times),\n",
        "            \"errors\": sum(1 for h in self.history if 'error' in h)\n",
        "        }\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history\n",
        "    \n",
        "    def generate_recommendation_stream(query: str, retrieved: List[Dict]) -> str:\n",
        "        \"\"\"Generate recommendation with streaming output.\"\"\"\n",
        "        if not gemini_client:\n",
        "            return \"Gemini not configured\"\n",
        "        \n",
        "        # Prepare context\n",
        "        context = \"\\n\".join([\n",
        "            f\"{i}. {p['product_name']} - {p['price']} - {p['rating']}/5\"\n",
        "            for i, p in enumerate(retrieved[:3], 1)\n",
        "        ])\n",
        "        \n",
        "        prompt = f\"\"\"Recommend product for: {query}\n",
        "\n",
        "    Products:\n",
        "    {context}\n",
        "\n",
        "    Provide brief recommendation.\"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Use generate_content_stream for streaming\n",
        "            response_stream = gemini_client.models.generate_content_stream(\n",
        "                model=config.gemini_model,\n",
        "                contents=prompt,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=0.7,\n",
        "                    max_output_tokens=config.gemini_max_tokens\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            # Stream and display chunks\n",
        "            full_response = \"\"\n",
        "            print(\"\\nRecommendation:\")\n",
        "            print(\"-\" * 80)\n",
        "            \n",
        "            for chunk in response_stream:\n",
        "                if chunk.text:\n",
        "                    print(chunk.text, end=\"\", flush=True)\n",
        "                    full_response += chunk.text\n",
        "            \n",
        "            print(\"\\n\" + \"-\" * 80)\n",
        "            return full_response\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    print(\"Streaming recommendation generator ready\")\n",
        "\n",
        "\n",
        "chatbot = RAGChatBot()\n",
        "print(\"\\nChatbot ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContinuousRAGChat:\n",
        "    \"\"\"Continuous chat with context and streaming.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.chat_session = None\n",
        "        self.history = []\n",
        "        self.initialize_chat()\n",
        "    \n",
        "    def initialize_chat(self):\n",
        "        \"\"\"Initialize Gemini chat session.\"\"\"\n",
        "        if not gemini_client:\n",
        "            print(\"WARNING: Gemini not configured\")\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            # Create chat session\n",
        "            self.chat_session = gemini_client.chats.create(\n",
        "                model=config.gemini_model,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=0.7,\n",
        "                    max_output_tokens=config.gemini_max_tokens,\n",
        "                )\n",
        "            )\n",
        "            print(\"Chat session initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing chat: {e}\")\n",
        "    \n",
        "    def chat_stream(self, user_input: str, use_rag: bool = True):\n",
        "        \"\"\"Chat with streaming and optional RAG.\"\"\"\n",
        "        if not self.chat_session:\n",
        "            print(\"Chat session not initialized\")\n",
        "            return\n",
        "        \n",
        "        print(f\"\\nYou: {user_input}\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Step 1: RAG Retrieval (if enabled)\n",
        "        if use_rag:\n",
        "            print(\"Searching products...\")\n",
        "            start = time.time()\n",
        "            retrieved = hybrid_retrieve(user_input, top_k=5)\n",
        "            print(f\"Found {len(retrieved)} products in {time.time()-start:.2f}s\\n\")\n",
        "            \n",
        "            # Build context\n",
        "            context = \"Available products:\\n\"\n",
        "            for i, p in enumerate(retrieved[:3], 1):\n",
        "                context += f\"{i}. {p['product_name']} - {p['price']} - {p['rating']}/5\\n\"\n",
        "            \n",
        "            # Augment prompt with RAG context\n",
        "            augmented_prompt = f\"{context}\\n\\nUser question: {user_input}\"\n",
        "        else:\n",
        "            augmented_prompt = user_input\n",
        "        \n",
        "        # Step 2: Stream response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        \n",
        "        try:\n",
        "            response_stream = self.chat_session.send_message_stream(augmented_prompt)\n",
        "            \n",
        "            full_response = \"\"\n",
        "            for chunk in response_stream:\n",
        "                if chunk.text:\n",
        "                    print(chunk.text, end=\"\", flush=True)\n",
        "                    full_response += chunk.text\n",
        "            \n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            \n",
        "            # Save to history\n",
        "            self.history.append({\n",
        "                \"user\": user_input,\n",
        "                \"assistant\": full_response,\n",
        "                \"timestamp\": time.time()\n",
        "            })\n",
        "            \n",
        "            return full_response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nError: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def chat_no_stream(self, user_input: str, use_rag: bool = True):\n",
        "        \"\"\"Chat without streaming (instant response).\"\"\"\n",
        "        if not self.chat_session:\n",
        "            return \"Chat session not initialized\"\n",
        "        \n",
        "        print(f\"\\nYou: {user_input}\")\n",
        "        \n",
        "        if use_rag:\n",
        "            retrieved = hybrid_retrieve(user_input, top_k=3)\n",
        "            context = \"\\n\".join([\n",
        "                f\"{i}. {p['product_name']} - {p['price']}\"\n",
        "                for i, p in enumerate(retrieved[:3], 1)\n",
        "            ])\n",
        "            augmented = f\"Products:\\n{context}\\n\\nQuestion: {user_input}\"\n",
        "        else:\n",
        "            augmented = user_input\n",
        "        \n",
        "        try:\n",
        "            response = self.chat_session.send_message(augmented)\n",
        "            print(f\"Assistant: {response.text}\")\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def get_history(self):\n",
        "        \"\"\"Get chat history.\"\"\"\n",
        "        return self.history\n",
        "    \n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear chat history and restart session.\"\"\"\n",
        "        self.history = []\n",
        "        self.initialize_chat()\n",
        "        print(\"Chat history cleared, session restarted\")\n",
        "\n",
        "# Initialize continuous chat\n",
        "continuous_chat = ContinuousRAGChat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_chat_loop():\n",
        "    \"\"\"Interactive chat loop with streaming.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RAG CHATBOT - INTERACTIVE MODE\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nCommands:\")\n",
        "    print(\"  - Type your question for product recommendations\")\n",
        "    print(\"  - 'history' - Show conversation history\")\n",
        "    print(\"  - 'clear' - Clear chat history\")\n",
        "    print(\"  - 'quit' - Exit\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nYou: \").strip()\n",
        "            \n",
        "            if not user_input:\n",
        "                continue\n",
        "            \n",
        "            # Handle commands\n",
        "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"\\nGoodbye!\")\n",
        "                break\n",
        "            \n",
        "            elif user_input.lower() == 'history':\n",
        "                print(\"\\n--- Chat History ---\")\n",
        "                for i, item in enumerate(continuous_chat.get_history(), 1):\n",
        "                    print(f\"\\n{i}. You: {item['user']}\")\n",
        "                    print(f\"   Bot: {item['assistant'][:100]}...\")\n",
        "                continue\n",
        "            \n",
        "            elif user_input.lower() == 'clear':\n",
        "                continuous_chat.clear_history()\n",
        "                continue\n",
        "            \n",
        "            # Process query with streaming\n",
        "            continuous_chat.chat_stream(user_input, use_rag=True)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nInterrupted. Type 'quit' to exit.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError: {e}\")\n",
        "\n",
        "# Run interactive mode\n",
        "# interactive_chat_loop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running tests\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Test 1: Recommendation\n",
            "\n",
            "Query: wireless earbuds under 3000\n",
            "================================================================================\n",
            "Retrieving...\n",
            "Retrieved 5 in 0.30s\n",
            "\n",
            "Products:\n",
            "--------------------------------------------------------------------------------\n",
            "1. boAt Airdopes 141 Bluetooth Truly Wireless in Ear Earbuds with mic, 42H Playtime, Beast Mode(Low Latency Upto 80ms) for Gaming, ENx Tech, ASAP Charge, IWP, IPX4 Water Resistance (Bold Black)\n",
            "   Electronics|Headphones,Earbuds&Accessories|Headphones|In-Ear | 1499.0 | 3.9/5\n",
            "   Score: -0.2658\n",
            "\n",
            "2. boAt Airdopes 171 in Ear Bluetooth True Wireless Earbuds with Upto 13 Hours Battery, IPX4, Bluetooth v5.0, Dual Tone Finish with Mic (Mysterious Blue)\n",
            "   Electronics|Headphones,Earbuds&Accessories|Headphones|In-Ear | 1199.0 | 3.9/5\n",
            "   Score: -0.8701\n",
            "\n",
            "3. ZEBRONICS Zeb-Sound Bomb N1 True Wireless in Ear Earbuds with Mic ENC, Gaming Mode (up to 50ms), up to 18H Playback, BT V5.2, Fidget Case, Voice Assistant, Splash Proof, Type C (Midnight Black)\n",
            "   Electronics|Headphones,Earbuds&Accessories|Headphones|In-Ear | 999.0 | 3.5/5\n",
            "   Score: -1.0268\n",
            "\n",
            "4. Wecool Moonwalk M1 ENC True Wireless in Ear Earbuds with Mic, Titanium Drivers for Rich Bass Experience, 40+ Hours Play Time, Type C Fast Charging, Low Latency, BT 5.3, IPX5, Deep Bass (Black)\n",
            "   Electronics|Headphones,Earbuds&Accessories|Headphones|In-Ear | 889.0 | 4.2/5\n",
            "   Score: -1.1727\n",
            "\n",
            "5. boAt Airdopes 121v2 in-Ear True Wireless Earbuds with Upto 14 Hours Playback, 8MM Drivers, Battery Indicators, Lightweight Earbuds & Multifunction Controls (Active Black, with Mic)\n",
            "   Electronics|Headphones,Earbuds&Accessories|Headphones|In-Ear | 1299.0 | 3.8/5\n",
            "   Score: -1.3522\n",
            "\n",
            "Generating...\n",
            "\n",
            "Recommendation:\n",
            "--------------------------------------------------------------------------------\n",
            "Dựa trên truy vấn \"tai nghe không dây dưới 3000\" và các sản phẩm được cung cấp, dưới đây là đề xuất:\n",
            "\n",
            "### 1. Tóm tắt ngắn gọn\n",
            "Bạn đang tìm kiếm tai nghe không dây (wireless earbuds) với ngân sách dưới 3000. Tất cả các sản phẩm được cung cấp đều là tai nghe nhét tai không dây và nằm trong tầm giá này, mang đến nhiều lựa chọn về pin, tính năng chơi game và khả năng chống ồn.\n",
            "\n",
            "### 2. Top 2 đề xuất với lý do\n",
            "\n",
            "1.  **boAt Airdopes 141 Bluetooth Truly Wireless in Ear Earbuds**\n",
            "    *   **Lý do:** Đây là một lựa chọn rất phổ biến và đáng tin cậy. Với hơn 136.000 lượt đánh giá và mức điểm 3.9/5, sản phẩm này chứng tỏ được chất lượng và sự hài lòng của người dùng. Thời lượng pin 42 giờ rất ấn tượng, cùng với chế độ Beast Mode (độ trễ thấp 80ms) phù hợp cho game thủ và công nghệ ASAP Charge giúp sạc nhanh. Giá cả rất phải chăng (1499.0).\n",
            "\n",
            "2.  **Wecool Moonwalk M1 ENC True Wireless in Ear Earbuds**\n",
            "    *   **Lý do:** Sản phẩm này có mức đánh giá cao nhất trong danh sách (4.2/5) và mức giá cực kỳ cạnh tranh (889.0). Nó nổi bật với tính năng Chống ồn môi trường (ENC) cho cuộc gọi rõ ràng, thời lượng pin hơn 40 giờ, trình điều khiển Titanium cho âm trầm mạnh mẽ và chế độ độ trễ thấp phù hợp cho chơi game. Khả năng chống nước IPX5 cũng là một điểm cộng.\n",
            "\n",
            "### 3. Các tính năng chính\n",
            "\n",
            "*   **Thời lượng pin dài:** Nhiều lựa chọn có thời lượng phát nhạc lên đến 40-42 giờ (bao gồm cả hộp sạc).\n",
            "*   **Chế độ chơi game (Low Latency):** Một số tai nghe có chế độ độ trễ thấp (80ms, 50ms) giúp đồng bộ hóa âm thanh tốt hơn khi chơi game.\n",
            "*   **Chống ồn môi trường (ENC):** Một số mẫu tích hợp ENC để cải thiện chất lượng cuộc gọi bằng cách loại bỏ tiếng ồn xung quanh.\n",
            "*   **Chống nước/mồ hôi (IPX4, IPX5):** Phù hợp cho việc tập luyện hoặc sử dụng hàng ngày mà không lo ngại mồ hôi hay nước bắn.\n",
            "*   **Sạc nhanh:** Tính năng ASAP Charge hoặc Type C Fast Charging giúp sạc pin nhanh chóng.\n",
            "*   **Điều khiển cảm ứng:** Hầu hết các mẫu đều có điều khiển cảm ứng dễ sử dụng.\n",
            "*   **Bluetooth v5.0/v5.2/v5.3:** Đảm bảo kết nối ổn định và hiệu quả năng lượng.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generation: 7.82s\n",
            "\n",
            "\n",
            "Test 2: Q&A\n",
            "\n",
            "Query: What are best Bluetooth speakers?\n",
            "================================================================================\n",
            "Retrieving...\n",
            "Retrieved 5 in 0.28s\n",
            "\n",
            "Products:\n",
            "--------------------------------------------------------------------------------\n",
            "1. Infinity (JBL Fuze 100, Wireless Portable Bluetooth Speaker with Mic, Deep Bass, Dual Equalizer, IPX7 Waterproof, Rugged Fabric Design (Black)\n",
            "   Electronics|HomeAudio|Speakers|OutdoorSpeakers | 1499.0 | 4.1/5\n",
            "   Score: 2.6492\n",
            "\n",
            "2. Infinity (JBL Fuze Pint, Wireless Ultra Portable Mini Speaker with Mic, Deep Bass, Dual Equalizer, Bluetooth 5.0 with Voice Assistant Support for Mobiles (Black)\n",
            "   Electronics|HomeAudio|Speakers|OutdoorSpeakers | 899.0 | 4.1/5\n",
            "   Score: 2.2040\n",
            "\n",
            "3. JBL Go 2, Wireless Portable Bluetooth Speaker with Mic, JBL Signature Sound, Vibrant Color Options with IPX7 Waterproof & AUX (Blue)\n",
            "   Electronics|HomeAudio|Speakers|BluetoothSpeakers | 1999.0 | 4.3/5\n",
            "   Score: 1.6252\n",
            "\n",
            "4. Zebronics ZEB-COUNTY 3W Wireless Bluetooth Portable Speaker With Supporting Carry Handle, USB, SD Card, AUX, FM & Call Function. (Green)\n",
            "   Electronics|HomeAudio|Speakers|BluetoothSpeakers | 549.0 | 3.9/5\n",
            "   Score: 0.9361\n",
            "\n",
            "5. boAt Stone 180 5W Bluetooth Speaker with Upto 10 Hours Playback, 1.75\" Driver, IPX7 & TWS Feature(Black)\n",
            "   Electronics|HomeAudio|Speakers|OutdoorSpeakers | 999.0 | 4.1/5\n",
            "   Score: 0.6930\n",
            "\n",
            "Answering...\n",
            "\n",
            "Answer:\n",
            "--------------------------------------------------------------------------------\n",
            "Dựa trên các sản phẩm có sẵn, dưới đây là một số lựa chọn loa Bluetooth tốt nhất, tùy thuộc vào ưu tiên của bạn:\n",
            "\n",
            "1.  **JBL Go 2 (Giá: 1999.0, Đánh giá: 4.3/5)**\n",
            "    *   **Điểm nổi bật:** Có đánh giá cao nhất trong danh sách, sở hữu âm thanh đặc trưng của JBL, thiết kế chống nước IPX7 và micrô chống ồn tích hợp. Đây là lựa chọn tuyệt vời nếu bạn ưu tiên chất lượng âm thanh và độ bền.\n",
            "    *   **Lưu ý:** Thời lượng pin 5 giờ.\n",
            "\n",
            "2.  **boAt Stone 180 5W Bluetooth Speaker (Giá: 999.0, Đánh giá: 4.1/5)**\n",
            "    *   **Điểm nổi bật:** Cung cấp thời lượng phát nhạc lên đến 10 giờ (lâu nhất trong danh sách), có trình điều khiển động 1.75 inch cho âm thanh mạnh mẽ, chống nước IPX7 và tính năng TWS (kết nối hai loa để tạo âm thanh nổi). Đây là lựa chọn tuyệt vời cho những ai cần pin \"trâu\" và âm thanh sống động.\n",
            "\n",
            "3.  **Infinity (JBL Fuze 100) (Giá: 1499.0, Đánh giá: 4.1/5)**\n",
            "    *   **Điểm nổi bật:** Loa di động nhỏ gọn với tính năng Dual Equalizer cho chế độ âm trầm sâu (Deep Bass) hoặc bình thường. Thời lượng pin 9 giờ khá tốt và khả năng chống nước IPX7. Phù hợp cho những ai thích âm trầm mạnh mẽ và sự tiện lợi.\n",
            "\n",
            "**Tóm lại:**\n",
            "\n",
            "*   Nếu bạn ưu tiên **chất lượng âm thanh và độ bền** (chống nước), **JBL Go 2** là lựa chọn hàng đầu.\n",
            "*   Nếu bạn cần **thời lượng pin dài nhất và âm thanh mạnh mẽ** với mức giá tốt, hãy chọn **boAt Stone 180**.\n",
            "*   Nếu bạn muốn một loa **di động với âm trầm sâu** và thời lượng pin tốt, **Infinity (JBL Fuze 100)** là một lựa chọn đáng cân nhắc.\n",
            "\n",
            "Các sản phẩm còn lại như Infinity (JBL Fuze Pint) và Zebronics ZEB-COUNTY phù hợp hơn cho những ai tìm kiếm loa cực kỳ nhỏ gọn hoặc giá cả phải chăng nhất, nhưng có thể không sánh bằng về hiệu suất tổng thể.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generation: 9.98s\n",
            "\n",
            "\n",
            "Summary\n",
            "================================================================================\n",
            "Queries: 4\n",
            "Avg retrieval: 0.29s\n",
            "Avg generation: 5.74s\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nRunning tests\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nTest 1: Recommendation\")\n",
        "test1 = chatbot.chat(\"wireless earbuds under 3000\", mode='recommend')\n",
        "\n",
        "print(\"\\n\\nTest 2: Q&A\")\n",
        "test2 = chatbot.chat(\"What are best Bluetooth speakers?\", mode='qa')\n",
        "\n",
        "print(\"\\n\\nSummary\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Queries: {len(chatbot.history)}\")\n",
        "retrieval_times = [r.get('retrieval_time', 0) for r in chatbot.history if 'retrieval_time' in r]\n",
        "avg_ret = np.mean(retrieval_times) if retrieval_times else 0\n",
        "avg_gen = np.mean([r.get('generation_time', 0) for r in chatbot.history])\n",
        "print(f\"Avg retrieval: {avg_ret:.2f}s\")\n",
        "print(f\"Avg generation: {avg_gen:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Model Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save function ready. Call: save_all_models()\n"
          ]
        }
      ],
      "source": [
        "def save_all_models():\n",
        "    \"\"\"Save all models and components.\"\"\"\n",
        "    print(\"Saving models...\")\n",
        "    \n",
        "    # FAISS\n",
        "    path = os.path.join(config.model_dir, \"faiss_index.bin\")\n",
        "    faiss.write_index(faiss_index, path)\n",
        "    print(f\"Saved FAISS: {path}\")\n",
        "    \n",
        "    # Embeddings\n",
        "    path = os.path.join(config.model_dir, \"product_embeddings.npy\")\n",
        "    np.save(path, product_embeddings)\n",
        "    print(f\"Saved embeddings: {path}\")\n",
        "    \n",
        "    # TF-IDF vectorizer\n",
        "    path = os.path.join(config.model_dir, \"tfidf_vectorizer.pkl\")\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(tfidf_vectorizer, f)\n",
        "    print(f\"Saved TF-IDF vec: {path}\")\n",
        "    \n",
        "    # TF-IDF matrix\n",
        "    path = os.path.join(config.model_dir, \"tfidf_matrix.pkl\")\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(tfidf_vectors, f)\n",
        "    print(f\"Saved TF-IDF mat: {path}\")\n",
        "    \n",
        "    # Metadata\n",
        "    metadata = {\n",
        "        \"total_products\": len(df),\n",
        "        \"embedding_model\": config.embedding_model,\n",
        "        \"reranker_model\": config.reranker_model,\n",
        "        \"embedding_dim\": config.embedding_dim,\n",
        "        \"hybrid_alpha\": config.hybrid_alpha\n",
        "    }\n",
        "    path = os.path.join(config.model_dir, \"metadata.json\")\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"Saved metadata: {path}\")\n",
        "    \n",
        "    print(f\"\\nAll saved to: {config.model_dir}\")\n",
        "\n",
        "# save_all_models()\n",
        "print(\"Save function ready. Call: save_all_models()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Usage Summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: best keyboards?\n",
            "================================================================================\n",
            "Retrieving...\n",
            "Answering...\n",
            "\n",
            "Chào bạn, để xác định \"bàn phím tốt nhất\" còn tùy thuộc vào nhu cầu sử dụng của bạn (chơi game, làm việc văn phòng, sử dụng hàng ngày) và ngân sách. Dựa trên các sản phẩm có sẵn, đây là những lựa chọn hàng đầu:\n",
            "\n",
            "**1. Tốt nhất cho Gaming và Tính năng cao cấp:**\n",
            "\n",
            "*   **HP K500F Backlit Membrane Wired Gaming Keyboard**\n",
            "    *   **Giá:** 1149.0\n",
            "    *   **Đánh giá:** 4.3/5 (Cao nhất)\n",
            "    *   **Điểm nổi bật:** Đây là lựa chọn tốt nhất nếu bạn cần một bàn phím chơi game. Nó có đèn nền LED nhiều màu, tấm kim loại chắc chắn, 26 phím chống ghosting (anti-ghosting) và phím khóa Windows, rất quan trọng cho game thủ. Kèm theo bảo hành 3 năm.\n",
            "\n",
            "**2. Tốt nhất cho Sử dụng hàng ngày/Văn phòng (Yên tĩnh và Tiện lợi):**\n",
            "\n",
            "*   **Dell KB216 Wired Multimedia USB Keyboard**\n",
            "    *   **Giá:** 549.0\n",
            "    *   **Đánh giá:** 4.3/5 (Cao nhất)\n",
            "    *   **Điểm nổi bật:** Nếu bạn cần một bàn phím đáng tin cậy cho công việc văn phòng hoặc sử dụng hàng ngày, Dell KB216 là một lựa chọn tuyệt vời. Nó có các phím kiểu Chiclet siêu yên tĩnh, khả năng chống tràn và các phím nóng đa phương tiện tiện lợi (điều chỉnh âm lượng, phát/dừng).\n",
            "\n",
            "**3. Lựa chọn giá cả phải chăng với chất lượng khá:**\n",
            "\n",
            "*   **Quantum QHM-7406 Full-Sized Keyboard**\n",
            "    *   **Giá:** 299.0\n",
            "    *   **Đánh giá:** 3.8/5\n",
            "    *   **Điểm nổi bật:** Với mức giá rất phải chăng, bàn phím này vẫn cung cấp đủ tính năng cơ bản, có biểu tượng đồng Rupee, các phím nóng và đặc biệt là khả năng chống tràn, tuổi thọ lên đến 10 triệu lần nhấn phím.\n",
            "\n",
            "**Tóm tắt:**\n",
            "\n",
            "*   Nếu bạn là game thủ và muốn trải nghiệm tốt nhất, hãy chọn **HP K500F**.\n",
            "*   Nếu bạn cần một bàn phím đáng tin cậy, yên tĩnh và có tính năng đa phương tiện cho công việc hoặc sử dụng chung, **Dell KB216** là lựa chọn tuyệt vời.\n",
            "*   Nếu ngân sách eo hẹp, **Quantum QHM-7406** là một lựa chọn kinh tế tốt.\n"
          ]
        }
      ],
      "source": [
        "# Usage: chatbot.chat('query', mode='recommend') or mode='qa'\"\n",
        "# ## Recommendation\n",
        "# result = chatbot.chat(\"wireless earbuds\", mode='recommend')\n",
        "\n",
        "# ## Q&A\n",
        "# result = chatbot.chat(\"best keyboards?\", mode='qa')\n",
        "\n",
        "# ## Direct Retrieval\n",
        "# products = hybrid_retrieve(\"gaming mouse\", top_k=5)\n",
        "\n",
        "result = chatbot.chat(\"best keyboards?\", mode='qa')\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17.UI chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_interface(message, history):\n",
        "    \"\"\"Gradio chat interface.\"\"\"\n",
        "    # Retrieve products\n",
        "    retrieved = hybrid_retrieve(message, top_k=3)\n",
        "    \n",
        "    # Build context\n",
        "    context = \"\\n\".join([\n",
        "        f\"{i}. {p['product_name']} - {p['price']}\"\n",
        "        for i, p in enumerate(retrieved[:3], 1)\n",
        "    ])\n",
        "    \n",
        "    # Generate response\n",
        "    prompt = f\"Products:\\n{context}\\n\\nQuestion: {message}\"\n",
        "    \n",
        "    try:\n",
        "        response_stream = gemini_client.models.generate_content_stream(\n",
        "            model=config.gemini_model,\n",
        "            contents=prompt,\n",
        "            config=types.GenerateContentConfig(temperature=0.7)\n",
        "        )\n",
        "        \n",
        "        # Accumulate streaming response\n",
        "        full_response = \"\"\n",
        "        for chunk in response_stream:\n",
        "            if chunk.text:\n",
        "                full_response += chunk.text\n",
        "                # Yield for real-time update in Gradio\n",
        "                yield full_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        yield f\"Error: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    chat_interface,\n",
        "    chatbot=gr.Chatbot(height=500),\n",
        "    textbox=gr.Textbox(placeholder=\"Ask about products...\", container=False, scale=7),\n",
        "    title=\"RAG Product Recommendation Chatbot\",\n",
        "    description=\"Ask questions about products and get AI-powered recommendations\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\n",
        "        \"I need wireless earbuds under 3000\",\n",
        "        \"What are the best gaming keyboards?\",\n",
        "        \"Show me budget Bluetooth speakers\"\n",
        "    ],\n",
        "    cache_examples=False,\n",
        ")\n",
        "\n",
        "# Launch\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
