{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ea1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Imports & Config (Notebook) ====\n",
    "import os, re, json, math, warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import dump, load\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Config cho Notebook ----\n",
    "@dataclass\n",
    "class NBConfig:\n",
    "    data_path: str = \"../data/processed/amazon.csv\"\n",
    "    model_dir: str = \"../models/recommendation\"\n",
    "    results_dir: str = \"../results\"\n",
    "    seed: int = 42\n",
    "    min_interactions: int = 3\n",
    "    n_factors: int = 64\n",
    "    alpha: float = 0.55\n",
    "    tfidf_max_features: int = 20000\n",
    "    tfidf_ngram_min: int = 1\n",
    "    tfidf_ngram_max: int = 2\n",
    "    top_k_eval: int = 10\n",
    "    eval_sample_users: int = 100\n",
    "    center_by_user: bool = True  # nên bật\n",
    "\n",
    "CFG = NBConfig()  # chỉnh tại đây nếu cần\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def safe_str(x) -> str:\n",
    "    if pd.isna(x): return \"\"\n",
    "    x = re.sub(r\"\\s+\", \" \", str(x))\n",
    "    return x.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68431104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>HitRate@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NumUsers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Content-Based</td>\n",
       "      <td>0.544271</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hybrid</td>\n",
       "      <td>0.153654</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Collaborative</td>\n",
       "      <td>0.028256</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Popularity</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model   NDCG@10  HitRate@10  Recall@10  NumUsers\n",
       "2  Content-Based  0.544271        0.78       0.78       100\n",
       "4         Hybrid  0.153654        0.39       0.39       100\n",
       "3  Collaborative  0.028256        0.06       0.06       100\n",
       "1         Random  0.007317        0.02       0.02       100\n",
       "0     Popularity  0.000000        0.00       0.00       100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../results\\recommendation_results.csv and model to ../models/recommendation\n"
     ]
    }
   ],
   "source": [
    "# ==== Data load & prep ====\n",
    "def load_and_prepare(cfg: NBConfig) -> pd.DataFrame:\n",
    "    df = pd.read_csv(cfg.data_path)\n",
    "    if \"user_id\" in df.columns and df[\"user_id\"].dtype == object and df[\"user_id\"].astype(str).str.contains(\",\").any():\n",
    "        tmp = df.copy()\n",
    "        tmp[\"user_id_list\"] = tmp[\"user_id\"].astype(str).str.split(\",\")\n",
    "        if \"review_id\" in tmp.columns:\n",
    "            tmp[\"review_id_list\"] = tmp[\"review_id\"].astype(str).str.split(\",\")\n",
    "            tmp = tmp.explode([\"user_id_list\", \"review_id_list\"])\n",
    "            tmp[\"review_id\"] = tmp[\"review_id_list\"].astype(str).str.strip()\n",
    "            tmp.drop(columns=[\"review_id_list\"], inplace=True)\n",
    "        else:\n",
    "            tmp = tmp.explode([\"user_id_list\"])\n",
    "        tmp[\"user_id\"] = tmp[\"user_id_list\"].astype(str).str.strip()\n",
    "        df = tmp.drop(columns=[\"user_id_list\"], errors=\"ignore\")\n",
    "\n",
    "    need = [\"user_id\",\"product_id\",\"rating\",\"category\",\"about_product\"]\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing: raise ValueError(f\"Missing cols: {missing}\")\n",
    "\n",
    "    inter = df[need].copy()\n",
    "    inter = inter.dropna(subset=[\"user_id\",\"product_id\",\"rating\"])\n",
    "    inter[\"user_id\"] = inter[\"user_id\"].astype(str).str.strip()\n",
    "    inter[\"product_id\"] = inter[\"product_id\"].astype(str).str.strip()\n",
    "    inter[\"rating\"] = pd.to_numeric(inter[\"rating\"], errors=\"coerce\")\n",
    "    inter = inter.dropna(subset=[\"rating\"]).drop_duplicates(subset=[\"user_id\",\"product_id\"])\n",
    "\n",
    "    # Filter users with >= min_interactions\n",
    "    uc = inter.groupby(\"user_id\").size()\n",
    "    valid_users = uc[uc >= cfg.min_interactions].index\n",
    "    inter = inter[inter[\"user_id\"].isin(valid_users)].copy()\n",
    "    return inter\n",
    "\n",
    "# ==== Leave-One-Out split ====\n",
    "def leave_one_out_split(interactions: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    jitter = np.random.uniform(0, 1e-6, size=len(interactions))\n",
    "    tmp = interactions.copy()\n",
    "    tmp[\"__j\"] = jitter\n",
    "    train, test = [], []\n",
    "    for uid, g in tmp.groupby(\"user_id\", sort=False):\n",
    "        g = g.sort_values([\"rating\",\"__j\"], ascending=[False, True])\n",
    "        if len(g) >= 2:\n",
    "            train.append(g.iloc[:-1])\n",
    "            test.append(g.iloc[-1:])\n",
    "        else:\n",
    "            train.append(g)\n",
    "    return pd.concat(train, ignore_index=True), (pd.concat(test, ignore_index=True) if len(test) else pd.DataFrame())\n",
    "\n",
    "# ==== Encoders & Matrix ====\n",
    "def build_encoders(train_df: pd.DataFrame):\n",
    "    ue, pe = LabelEncoder(), LabelEncoder()\n",
    "    ue.fit(train_df[\"user_id\"].unique())\n",
    "    pe.fit(train_df[\"product_id\"].unique())\n",
    "    return ue, pe\n",
    "\n",
    "def build_upm(train_df: pd.DataFrame, ue: LabelEncoder, pe: LabelEncoder, center_by_user=True):\n",
    "    td = train_df.copy()\n",
    "    td[\"u\"] = ue.transform(td[\"user_id\"])\n",
    "    td[\"i\"] = pe.transform(td[\"product_id\"])\n",
    "    vals = td[\"rating\"].astype(float).values.copy()\n",
    "    if center_by_user:\n",
    "        vals = vals - td.groupby(\"u\")[\"rating\"].transform(\"mean\").values\n",
    "    upm = csr_matrix((vals, (td[\"u\"].values, td[\"i\"].values)),\n",
    "                     shape=(len(ue.classes_), len(pe.classes_)))\n",
    "    return upm\n",
    "\n",
    "# ==== Popularity (Bayesian smoothing) ====\n",
    "def build_popularity(train_df: pd.DataFrame) -> List[str]:\n",
    "    pop = (train_df.groupby(\"product_id\")\n",
    "           .agg(mean_rating=(\"rating\",\"mean\"), count=(\"rating\",\"count\"))\n",
    "           .reset_index())\n",
    "    gmean, m = train_df[\"rating\"].mean(), 5\n",
    "    pop[\"bayes_mean\"] = (pop[\"count\"]*pop[\"mean_rating\"] + m*gmean)/(pop[\"count\"]+m)\n",
    "    pop[\"score\"] = pop[\"bayes_mean\"] * np.log1p(pop[\"count\"])\n",
    "    pop = pop.sort_values(\"score\", ascending=False)\n",
    "    return pop[\"product_id\"].tolist()\n",
    "\n",
    "# ==== Content model ====\n",
    "def build_content(train_df: pd.DataFrame, cfg: NBConfig):\n",
    "    meta = train_df[[\"product_id\",\"category\",\"about_product\"]].drop_duplicates(\"product_id\").copy()\n",
    "    meta[\"category\"] = meta[\"category\"].map(safe_str)\n",
    "    meta[\"about_product\"] = meta[\"about_product\"].map(safe_str)\n",
    "    meta[\"combined\"] = (meta[\"category\"] + \" \" + meta[\"about_product\"]).str.lower()\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=cfg.tfidf_max_features,\n",
    "                            ngram_range=(cfg.tfidf_ngram_min, cfg.tfidf_ngram_max),\n",
    "                            stop_words=\"english\")\n",
    "    X = tfidf.fit_transform(meta[\"combined\"])\n",
    "    sim = cosine_similarity(X, X)\n",
    "    pid2idx = {pid:i for i, pid in enumerate(meta[\"product_id\"].tolist())}\n",
    "    idx2pid = {i:pid for pid, i in pid2idx.items()}\n",
    "    return {\"meta\":meta, \"tfidf\":tfidf, \"X\":X, \"sim\":sim, \"pid2idx\":pid2idx, \"idx2pid\":idx2pid}\n",
    "\n",
    "def content_recommend(pid: str, art: Dict[str,Any], k: int, pop_rank: List[str]) -> List[str]:\n",
    "    if pid not in art[\"pid2idx\"]: return pop_rank[:k]\n",
    "    idx = art[\"pid2idx\"][pid]\n",
    "    scores = list(enumerate(art[\"sim\"][idx]))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [art[\"idx2pid\"][i] for i,_ in scores[1:k+1]]\n",
    "\n",
    "# ==== Collaborative (SVD) ====\n",
    "def build_svd(upm: csr_matrix, n_factors: int, seed: int):\n",
    "    svd = TruncatedSVD(n_components=n_factors, random_state=seed)\n",
    "    U = svd.fit_transform(upm)\n",
    "    V = svd.components_.T\n",
    "    return svd, U, V\n",
    "\n",
    "def collab_recommend(uid: str, enc: Dict[str,Any], U: np.ndarray, V: np.ndarray,\n",
    "                     upm: csr_matrix, k: int, pop_rank: List[str]) -> List[str]:\n",
    "    ue, pe = enc[\"user\"], enc[\"product\"]\n",
    "    if uid not in ue.classes_: return pop_rank[:k]\n",
    "    uidx = ue.transform([uid])[0]\n",
    "    interacted = upm[uidx].nonzero()[1]\n",
    "    scores = U[uidx] @ V.T\n",
    "    if len(interacted) > 0:\n",
    "        scores = scores.copy()\n",
    "        scores[interacted] = -np.inf\n",
    "    best = np.argsort(scores)[::-1][:k]\n",
    "    return pe.inverse_transform(best).tolist()\n",
    "\n",
    "# ==== Hybrid ====\n",
    "def hybrid_recommend(uid: str, train_df: pd.DataFrame, enc: Dict[str,Any],\n",
    "                     U: np.ndarray, V: np.ndarray, upm: csr_matrix,\n",
    "                     content_art: Dict[str,Any], alpha: float, k: int,\n",
    "                     pop_rank: List[str]) -> List[str]:\n",
    "    collab_list = collab_recommend(uid, enc, U, V, upm, max(50,k), pop_rank)\n",
    "    hist = train_df[train_df[\"user_id\"] == uid]\n",
    "    liked = hist[hist[\"rating\"] >= 4][\"product_id\"].tolist() or hist[\"product_id\"].tolist()\n",
    "    content_scores = {}\n",
    "    for pid in liked[:10]:\n",
    "        for p in content_recommend(pid, content_art, 50, pop_rank):\n",
    "            content_scores[p] = content_scores.get(p, 0.0) + 1.0\n",
    "    if content_scores:\n",
    "        mx = max(content_scores.values())\n",
    "        for p in list(content_scores.keys()):\n",
    "            content_scores[p] /= mx\n",
    "    hybrid = {}\n",
    "    for i, p in enumerate(collab_list):\n",
    "        hybrid[p] = hybrid.get(p, 0.0) + alpha * (1.0/(i+1))\n",
    "    for p, s in content_scores.items():\n",
    "        hybrid[p] = hybrid.get(p, 0.0) + (1.0 - alpha) * s\n",
    "    interacted = set(hist[\"product_id\"].tolist())\n",
    "    hybrid = {p:s for p,s in hybrid.items() if p not in interacted}\n",
    "    return [p for p,_ in sorted(hybrid.items(), key=lambda x:x[1], reverse=True)[:k]]\n",
    "\n",
    "# ==== Metrics & Eval ====\n",
    "def ndcg_at_k(rec: List[str], rel: List[str], k: int) -> float:\n",
    "    rec = rec[:k]\n",
    "    dcg = 0.0\n",
    "    for i, it in enumerate(rec):\n",
    "        if it in rel: dcg += 1.0 / math.log2(i+2)\n",
    "    idcg = sum(1.0 / math.log2(i+2) for i in range(min(len(rel), k)))\n",
    "    return float(dcg/idcg) if idcg > 0 else 0.0\n",
    "\n",
    "def hitrate_at_k(rec: List[str], rel: List[str], k: int) -> float:\n",
    "    return float(any(it in rel for it in rec[:k]))\n",
    "\n",
    "def recall_at_k(rec: List[str], rel: List[str], k: int) -> float:\n",
    "    if len(rel)==0: return 0.0\n",
    "    return float(len(set(rec[:k]) & set(rel)) / len(rel))\n",
    "\n",
    "def evaluate_all(train_df, test_df, enc, U, V, upm, content_art, pop_rank, cfg: NBConfig) -> pd.DataFrame:\n",
    "    users = list(set(train_df[\"user_id\"]) & set(test_df[\"user_id\"]))\n",
    "    if len(users) > cfg.eval_sample_users:\n",
    "        users = list(np.random.choice(users, size=cfg.eval_sample_users, replace=False))\n",
    "    all_pids = train_df[\"product_id\"].unique().tolist()\n",
    "    rng = np.random.default_rng(cfg.seed)\n",
    "\n",
    "    def do_eval(name, fn):\n",
    "        s_ndcg, s_hit, s_rec = [], [], []\n",
    "        for uid in users:\n",
    "            rel = test_df.loc[test_df[\"user_id\"] == uid, \"product_id\"].tolist()\n",
    "            if not rel: continue\n",
    "            recs = fn(uid)\n",
    "            s_ndcg.append(ndcg_at_k(recs, rel, cfg.top_k_eval))\n",
    "            s_hit.append(hitrate_at_k(recs, rel, cfg.top_k_eval))\n",
    "            s_rec.append(recall_at_k(recs, rel, cfg.top_k_eval))\n",
    "        return {\"Model\":name,\n",
    "                \"NDCG@10\": np.mean(s_ndcg) if s_ndcg else 0.0,\n",
    "                \"HitRate@10\": np.mean(s_hit) if s_hit else 0.0,\n",
    "                \"Recall@10\": np.mean(s_rec) if s_rec else 0.0,\n",
    "                \"NumUsers\": len(users)}\n",
    "\n",
    "    top10_pop = pop_rank[:cfg.top_k_eval]\n",
    "    rows = []\n",
    "    rows.append(do_eval(\"Popularity\", lambda uid: top10_pop))\n",
    "    rows.append(do_eval(\"Random\", lambda uid: rng.choice(all_pids, size=min(cfg.top_k_eval, len(all_pids)), replace=False).tolist()))\n",
    "    rows.append(do_eval(\"Content-Based\", lambda uid: content_recommend(\n",
    "        train_df[train_df[\"user_id\"]==uid].tail(1)[\"product_id\"].values[0]\n",
    "        if len(train_df[train_df[\"user_id\"]==uid]) else top10_pop[0],\n",
    "        content_art, cfg.top_k_eval, pop_rank)))\n",
    "    rows.append(do_eval(\"Collaborative\", lambda uid: collab_recommend(uid, enc, U, V, upm, cfg.top_k_eval, pop_rank)))\n",
    "    rows.append(do_eval(\"Hybrid\", lambda uid: hybrid_recommend(uid, train_df, enc, U, V, upm, content_art, cfg.alpha, cfg.top_k_eval, pop_rank)))\n",
    "    res = pd.DataFrame(rows).sort_values(\"NDCG@10\", ascending=False)\n",
    "    return res\n",
    "\n",
    "# ==== Save / Load artifacts ====\n",
    "def save_artifacts(model_dir: str, cfg: NBConfig, ue, pe, svd, U, V, upm_shape, content_art, pop_rank):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    dump({\n",
    "        \"config\": asdict(cfg),\n",
    "        \"user_encoder\": ue, \"product_encoder\": pe,\n",
    "        \"svd\": svd, \"U\": U, \"V\": V, \"upm_shape\": upm_shape,\n",
    "        \"content_pid2idx\": content_art[\"pid2idx\"],\n",
    "        \"content_idx2pid\": content_art[\"idx2pid\"],\n",
    "        \"content_tfidf\": content_art[\"tfidf\"],\n",
    "        \"content_X\": content_art[\"X\"],\n",
    "        \"content_similarity\": content_art[\"sim\"],\n",
    "        \"pop_rank\": pop_rank,\n",
    "    }, os.path.join(model_dir, \"hybrid_model.joblib\"))\n",
    "    with open(os.path.join(model_dir, \"manifest.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"n_users\": len(ue.classes_),\n",
    "            \"n_products\": len(pe.classes_),\n",
    "            \"n_factors\": cfg.n_factors,\n",
    "            \"alpha\": cfg.alpha,\n",
    "            \"tfidf_max_features\": cfg.tfidf_max_features\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_artifacts(model_dir: str) -> Dict[str,Any]:\n",
    "    return load(os.path.join(model_dir, \"hybrid_model.joblib\"))\n",
    "\n",
    "# ==== Run training in Notebook ====\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "interactions = load_and_prepare(CFG)\n",
    "train_df, test_df = leave_one_out_split(interactions)\n",
    "\n",
    "ue, pe = build_encoders(train_df)\n",
    "upm = build_upm(train_df, ue, pe, center_by_user=CFG.center_by_user)\n",
    "\n",
    "pop_rank = build_popularity(train_df)\n",
    "content_art = build_content(train_df, CFG)\n",
    "svd, U, V = build_svd(upm, CFG.n_factors, CFG.seed)\n",
    "\n",
    "enc = {\"user\": ue, \"product\": pe}\n",
    "results = evaluate_all(train_df, test_df, enc, U, V, upm, content_art, pop_rank, CFG)\n",
    "display(results)\n",
    "\n",
    "# Lưu kết quả & artifacts\n",
    "os.makedirs(CFG.results_dir, exist_ok=True)\n",
    "results_path = os.path.join(CFG.results_dir, \"recommendation_results.csv\")\n",
    "results.to_csv(results_path, index=False)\n",
    "\n",
    "save_artifacts(CFG.model_dir, CFG, ue, pe, svd, U, V, upm.shape, content_art, pop_rank)\n",
    "print(\"Saved:\", results_path, \"and model to\", CFG.model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d4cc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B0BMGG6NKT',\n",
       " 'B07WJWRNVK',\n",
       " 'B08B42LWKN',\n",
       " 'B088ZFJY82',\n",
       " 'B086Q3QMFS',\n",
       " 'B0859M539M',\n",
       " 'B084PJSSQ1',\n",
       " 'B083T5G5PM',\n",
       " 'B082LZGK39',\n",
       " 'B082LSVT4B']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Inference helpers (Notebook) ====\n",
    "def nb_recommend_for_user(user_id: str, model_dir: str = \"../model\", top_k: int = 10) -> List[str]:\n",
    "    art = load_artifacts(model_dir)\n",
    "    ue, pe = art[\"user_encoder\"], art[\"product_encoder\"]\n",
    "    U, V = art[\"U\"], art[\"V\"]\n",
    "    pop_rank = art[\"pop_rank\"]\n",
    "    if user_id not in ue.classes_:\n",
    "        return pop_rank[:top_k]\n",
    "    uidx = ue.transform([user_id])[0]\n",
    "    scores = U[uidx] @ V.T\n",
    "    best = np.argsort(scores)[::-1][:top_k]\n",
    "    return pe.inverse_transform(best).tolist()\n",
    "\n",
    "# ví dụ nhanh với 1 user trong train:\n",
    "some_user = train_df[\"user_id\"].iloc[0]\n",
    "nb_recommend_for_user(some_user, model_dir=CFG.model_dir, top_k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
