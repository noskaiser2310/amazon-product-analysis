{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34748ee",
   "metadata": {},
   "source": [
    "# **Amazon Sales Dataset - Recommendation System**\n",
    "\n",
    "**Objective**: Build and evaluate various recommendation models to identify the top-performing system.\n",
    "\n",
    "**Deliverables:**\n",
    "- Multiple recommendation models (Content-Based, Collaborative, Hybrid).\n",
    "- Model performance report (NDCG@10, HitRate@10, Recall@10).\n",
    "- A saved, reusable model artifact .\n",
    "- Prediction function for user recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a2e10",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef5626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import dump, load\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885578a5",
   "metadata": {},
   "source": [
    "### 2. Configure the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0378d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Notebook Configuration ----\n",
    "@dataclass\n",
    "class NBConfig:\n",
    "    data_path: str = \"../data/processed/amazon.csv\"  \n",
    "    model_dir: str = \"../models/recommendation\" \n",
    "    results_dir: str = \"../results\" # Directory to save evaluation results\n",
    "    seed: int = 42\n",
    "    min_interactions: int = 3 \n",
    "    n_factors: int = 64 # Number of latent factors for SVD\n",
    "    alpha: float = 0.2 # Weight for the Collaborative model in the Hybrid recommender\n",
    "    tfidf_max_features: int = 20000\n",
    "    tfidf_ngram_min: int = 1\n",
    "    tfidf_ngram_max: int = 2\n",
    "    top_k_eval: int = 10 # Evaluate the top 10 recommendations\n",
    "    eval_sample_users: int = 100 # Number of sample users for evaluation\n",
    "    center_by_user: bool = True  # Normalize ratings by subtracting the user's mean\n",
    "\n",
    "CFG = NBConfig()\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def safe_str(x) -> str:\n",
    "    if pd.isna(x): return \"\"\n",
    "    x = re.sub(r\"\\\\s+\", \" \", str(x))\n",
    "    return x.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a04c02",
   "metadata": {},
   "source": [
    "### 3. DATA LOADING AND PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db3df02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(cfg: NBConfig) -> pd.DataFrame:\n",
    "    df = pd.read_csv(cfg.data_path)\n",
    "    # \"Explode\" the user_id column to create distinct user-item interaction rows\n",
    "    if \"user_id\" in df.columns and df[\"user_id\"].dtype == object and df[\"user_id\"].astype(str).str.contains(\",\").any():\n",
    "        tmp = df.copy()\n",
    "        tmp[\"user_id_list\"] = tmp[\"user_id\"].astype(str).str.split(\",\")\n",
    "        tmp = tmp.explode([\"user_id_list\"])\n",
    "        tmp[\"user_id\"] = tmp[\"user_id_list\"].astype(str).str.strip()\n",
    "        df = tmp.drop(columns=[\"user_id_list\"], errors=\"ignore\")\n",
    "\n",
    "    need = [\"user_id\", \"product_id\", \"rating\", \"category_main\", \"about_product\", \"product_name\"]\n",
    "    df = df.dropna(subset=need)\n",
    "    df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"rating\"]).drop_duplicates(subset=[\"user_id\", \"product_id\"])\n",
    "\n",
    "    # Filter out users with few interactions to reduce noise\n",
    "    uc = df.groupby(\"user_id\").size()\n",
    "    valid_users = uc[uc >= cfg.min_interactions].index\n",
    "    df = df[df[\"user_id\"].isin(valid_users)].copy()\n",
    "    \n",
    "    # Reset index to ensure consistency\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "interactions = load_and_prepare(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07878ac0",
   "metadata": {},
   "source": [
    "### 4. EVALUATION STRATEGY: LEAVE-ONE-OUT SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb66b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1000 - Test set: 357.\n"
     ]
    }
   ],
   "source": [
    "def leave_one_out_split(interactions: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Add a small amount of jitter to ensure a unique sort order\n",
    "    jitter = np.random.uniform(0, 1e-6, size=len(interactions))\n",
    "    tmp = interactions.copy()\n",
    "    tmp[\"__j\"] = jitter\n",
    "    train, test = [], []\n",
    "    for uid, g in tmp.groupby(\"user_id\", sort=False):\n",
    "        # Sort by rating and jitter, taking the last item for the test set\n",
    "        g = g.sort_values([\"rating\", \"__j\"], ascending=[False, True])\n",
    "        if len(g) >= 2:\n",
    "            train.append(g.iloc[:-1])\n",
    "            test.append(g.iloc[-1:])\n",
    "        else:\n",
    "            train.append(g)\n",
    "    return pd.concat(train, ignore_index=True), (pd.concat(test, ignore_index=True) if len(test) else pd.DataFrame())\n",
    "\n",
    "train_df, test_df = leave_one_out_split(interactions)\n",
    "print(f\"Training set: {len(train_df)} - Test set: {len(test_df)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac1200",
   "metadata": {},
   "source": [
    "### 5. MODEL COMPONENT CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397927fc",
   "metadata": {},
   "source": [
    "#### 5.1. Encoders and User-Product Matrix (UPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64e709a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPM shape: (357, 239)\n"
     ]
    }
   ],
   "source": [
    "def build_encoders(train_df: pd.DataFrame):\n",
    "    ue, pe = LabelEncoder(), LabelEncoder()\n",
    "    ue.fit(train_df[\"user_id\"].unique())\n",
    "    pe.fit(train_df[\"product_id\"].unique())\n",
    "    return ue, pe\n",
    "\n",
    "def build_upm(train_df: pd.DataFrame, ue: LabelEncoder, pe: LabelEncoder, center_by_user=True):\n",
    "    td = train_df.copy()\n",
    "    td[\"u\"] = ue.transform(td[\"user_id\"])\n",
    "    td[\"i\"] = pe.transform(td[\"product_id\"])\n",
    "    vals = td[\"rating\"].astype(float).values.copy()\n",
    "    # Normalize ratings by subtracting the user's mean rating\n",
    "    if center_by_user:\n",
    "        vals = vals - td.groupby(\"u\")[\"rating\"].transform(\"mean\").values\n",
    "    upm = csr_matrix((vals, (td[\"u\"].values, td[\"i\"].values)),\n",
    "                     shape=(len(ue.classes_), len(pe.classes_)))\n",
    "    return upm\n",
    "\n",
    "ue, pe = build_encoders(train_df)\n",
    "upm = build_upm(train_df, ue, pe, center_by_user=CFG.center_by_user)\n",
    "print(f\"UPM shape: {upm.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d403e8",
   "metadata": {},
   "source": [
    "#### 5.2. Baseline Model: Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a8fa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_popularity(train_df: pd.DataFrame) -> List[str]:\n",
    "    # Calculate a popularity score combining mean rating and rating count\n",
    "    pop = (train_df.groupby(\"product_id\")\n",
    "           .agg(mean_rating=(\"rating\", \"mean\"), count=(\"rating\", \"count\"))\n",
    "           .reset_index())\n",
    "    gmean, m = train_df[\"rating\"].mean(), 5\n",
    "    pop[\"bayes_mean\"] = (pop[\"count\"] * pop[\"mean_rating\"] + m * gmean) / (pop[\"count\"] + m)\n",
    "    pop[\"score\"] = pop[\"bayes_mean\"] * np.log1p(pop[\"count\"])\n",
    "    pop = pop.sort_values(\"score\", ascending=False)\n",
    "    return pop[\"product_id\"].tolist()\n",
    "\n",
    "pop_rank = build_popularity(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823384b",
   "metadata": {},
   "source": [
    "#### 5.3. Content-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2baf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_content(train_df: pd.DataFrame, cfg: NBConfig):\n",
    "    # Create metadata for content-based model\n",
    "    meta = train_df[[\"product_id\", \"category_main\", \"about_product\", \"product_name\"]].drop_duplicates(\"product_id\").copy()\n",
    "    meta[\"combined\"] = (meta[\"category_main\"].map(safe_str) + \" \" + \n",
    "                        meta[\"about_product\"].map(safe_str) + \" \" +\n",
    "                        meta[\"product_name\"].map(safe_str)).str.lower()\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=cfg.tfidf_max_features, ngram_range=(cfg.tfidf_ngram_min, cfg.tfidf_ngram_max), stop_words=\"english\")\n",
    "    X = tfidf.fit_transform(meta[\"combined\"])\n",
    "    sim = cosine_similarity(X, X)\n",
    "    pid2idx = {pid: i for i, pid in enumerate(meta[\"product_id\"].tolist())}\n",
    "    idx2pid = {i: pid for pid, i in pid2idx.items()}\n",
    "    return {\"sim\": sim, \"pid2idx\": pid2idx, \"idx2pid\": idx2pid}\n",
    "\n",
    "content_artifacts = build_content(train_df, CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4b730",
   "metadata": {},
   "source": [
    "#### 5.4. Collaborative Filtering Model (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed856855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " U shape: (357, 64), V shape: (239, 64)\n"
     ]
    }
   ],
   "source": [
    "def build_svd(upm: csr_matrix, n_factors: int, seed: int):\n",
    "    svd = TruncatedSVD(n_components=n_factors, random_state=seed)\n",
    "    U = svd.fit_transform(upm)  # User-feature matrix\n",
    "    V = svd.components_.T       # Item-feature matrix\n",
    "    return svd, U, V\n",
    "\n",
    "svd, U, V = build_svd(upm, CFG.n_factors, CFG.seed)\n",
    "print(f\" U shape: {U.shape}, V shape: {V.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e6589",
   "metadata": {},
   "source": [
    "### 6. EVALUATING MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f4d72",
   "metadata": {},
   "source": [
    "#### 6.1. Recommendation and Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b388ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation functions for each model\n",
    "def recommend_content(uid: str, train_df: pd.DataFrame, content_art: Dict[str,Any], k: int, pop_rank: List[str]) -> List[str]:\n",
    "    # Use the last item the user interacted with as a base\n",
    "    last_pid = train_df[train_df[\"user_id\"] == uid].tail(1)[\"product_id\"].values\n",
    "    if not last_pid: return pop_rank[:k]\n",
    "    pid = last_pid[0]\n",
    "    \n",
    "    if pid not in content_art[\"pid2idx\"]: return pop_rank[:k]\n",
    "    idx = content_art[\"pid2idx\"][pid]\n",
    "    scores = list(enumerate(content_art[\"sim\"][idx]))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    interacted = set(train_df[train_df[\"user_id\"] == uid]['product_id'].tolist())\n",
    "    recs = [content_art[\"idx2pid\"][i] for i, _ in scores if content_art[\"idx2pid\"][i] not in interacted]\n",
    "    return recs[:k]\n",
    "\n",
    "def recommend_collab(uid: str, enc: Dict[str,Any], U: np.ndarray, V: np.ndarray, upm: csr_matrix, k: int, pop_rank: List[str]) -> List[str]:\n",
    "    ue, pe = enc[\"user\"], enc[\"product\"]\n",
    "    if uid not in ue.classes_: return pop_rank[:k]\n",
    "    uidx = ue.transform([uid])[0]\n",
    "    interacted = upm[uidx].nonzero()[1]\n",
    "    scores = U[uidx] @ V.T\n",
    "    if len(interacted) > 0:\n",
    "        scores = scores.copy()\n",
    "        scores[interacted] = -np.inf\n",
    "    best = np.argsort(scores)[::-1][:k]\n",
    "    return pe.inverse_transform(best).tolist()\n",
    "\n",
    "def recommend_hybrid(uid: str, train_df: pd.DataFrame, enc: Dict[str,Any], U: np.ndarray, V: np.ndarray, upm: csr_matrix, content_art: Dict[str,Any], alpha: float, k: int, pop_rank: List[str]) -> List[str]:\n",
    "    collab_list = recommend_collab(uid, enc, U, V, upm, k*5, pop_rank) # Get more candidates for re-ranking\n",
    "    \n",
    "    content_list = recommend_content(uid, train_df, content_art, k*5, pop_rank)\n",
    "    \n",
    "    # Calculate hybrid scores\n",
    "    hybrid_scores = {}\n",
    "    for i, p in enumerate(collab_list):\n",
    "        hybrid_scores[p] = hybrid_scores.get(p, 0.0) + alpha * (1.0 / (i + 1))\n",
    "    for i, p in enumerate(content_list):\n",
    "        hybrid_scores[p] = hybrid_scores.get(p, 0.0) + (1.0 - alpha) * (1.0 / (i + 1))\n",
    "        \n",
    "    return [p for p, _ in sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:k]]\n",
    "\n",
    "\n",
    "# Metric calculation functions\n",
    "def ndcg_at_k(rec: List[str], rel: List[str], k: int) -> float:\n",
    "    rec = rec[:k]\n",
    "    dcg = sum(1.0 / math.log2(i + 2) for i, it in enumerate(rec) if it in rel)\n",
    "    idcg = sum(1.0 / math.log2(i + 2) for i in range(min(len(rel), k)))\n",
    "    return float(dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "def hitrate_at_k(rec: List[str], rel: List[str], k: int) -> float:\n",
    "    return float(any(it in rel for it in rec[:k]))\n",
    "\n",
    "def recall_at_k(rec: List[str], rel: List[str], k: int) -> float:\n",
    "    if not rel: return 0.0\n",
    "    return float(len(set(rec[:k]) & set(rel)) / len(rel))\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_all(train_df, test_df, enc, U, V, upm, content_art, pop_rank, cfg: NBConfig) -> pd.DataFrame:\n",
    "    users = list(set(train_df[\"user_id\"]) & set(test_df[\"user_id\"]))\n",
    "    if len(users) > cfg.eval_sample_users:\n",
    "        users = list(np.random.choice(users, size=cfg.eval_sample_users, replace=False))\n",
    "    \n",
    "    all_pids = train_df[\"product_id\"].unique().tolist()\n",
    "    rng = np.random.default_rng(cfg.seed)\n",
    "\n",
    "    def do_eval(name, fn):\n",
    "        metrics = {'NDCG@10': [], 'HitRate@10': [], 'Recall@10': []}\n",
    "        for uid in users:\n",
    "            rel = test_df.loc[test_df[\"user_id\"] == uid, \"product_id\"].tolist()\n",
    "            if not rel: continue\n",
    "            recs = fn(uid)\n",
    "            metrics['NDCG@10'].append(ndcg_at_k(recs, rel, cfg.top_k_eval))\n",
    "            metrics['HitRate@10'].append(hitrate_at_k(recs, rel, cfg.top_k_eval))\n",
    "            metrics['Recall@10'].append(recall_at_k(recs, rel, cfg.top_k_eval))\n",
    "        return {\"Model\": name, **{k: np.mean(v) for k, v in metrics.items()}, \"NumUsers\": len(users)}\n",
    "\n",
    "    rows = [\n",
    "        do_eval(\"Popularity\", lambda uid: pop_rank[:cfg.top_k_eval]),\n",
    "        do_eval(\"Random\", lambda uid: rng.choice(all_pids, size=cfg.top_k_eval, replace=False).tolist()),\n",
    "        do_eval(\"Content-Based\", lambda uid: recommend_content(uid, train_df, content_art, cfg.top_k_eval, pop_rank)),\n",
    "        do_eval(\"Collaborative\", lambda uid: recommend_collab(uid, enc, U, V, upm, cfg.top_k_eval, pop_rank)),\n",
    "        do_eval(\"Hybrid\", lambda uid: recommend_hybrid(uid, train_df, enc, U, V, upm, content_art, cfg.alpha, cfg.top_k_eval, pop_rank))\n",
    "    ]\n",
    "    return pd.DataFrame(rows).sort_values(\"NDCG@10\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3eef6",
   "metadata": {},
   "source": [
    "#### 6.2. Run and Analyze Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db0eac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- MODEL PERFORMANCE EVALUATION RESULTS -----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>HitRate@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NumUsers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Content-Based</td>\n",
       "      <td>0.816818</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hybrid</td>\n",
       "      <td>0.815973</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Collaborative</td>\n",
       "      <td>0.026572</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Popularity</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model   NDCG@10  HitRate@10  Recall@10  NumUsers\n",
       "2  Content-Based  0.816818        0.86       0.86       100\n",
       "4         Hybrid  0.815973        0.86       0.86       100\n",
       "3  Collaborative  0.026572        0.04       0.04       100\n",
       "1         Random  0.014906        0.04       0.04       100\n",
       "0     Popularity  0.000000        0.00       0.00       100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results saved to: ../results\\recommendation_results.csv\n"
     ]
    }
   ],
   "source": [
    "enc = {\"user\": ue, \"product\": pe}\n",
    "results = evaluate_all(train_df, test_df, enc, U, V, upm, content_artifacts, pop_rank, CFG)\n",
    "\n",
    "print(\"\\n----- MODEL PERFORMANCE EVALUATION RESULTS -----\")\n",
    "display(results)\n",
    "\n",
    "# Save results\n",
    "os.makedirs(CFG.results_dir, exist_ok=True)\n",
    "results_path = os.path.join(CFG.results_dir, \"recommendation_results.csv\")\n",
    "results.to_csv(results_path, index=False)\n",
    "print(f\"\\nEvaluation results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d0478",
   "metadata": {},
   "source": [
    "### 7. SAVING AND USING THE MODEL FOR INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b999ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts saved to the '../models/recommendation' directory.\n",
      "\n",
      "----- Recommendations for User ID: AHCTC6ULH4XB6YHDY6PCH2R772LQ -----\n",
      "['B0BMGG6NKT', 'B07WJV6P1R', 'B088ZFJY82', 'B086Q3QMFS', 'B0859M539M', 'B084PJSSQ1', 'B083T5G5PM', 'B082LZGK39', 'B082LSVT4B', 'B081FJWN52', 'B081FG1QYX', 'B0811VCGL5', 'B07XLCFSSN', 'B07WJWRNVK', 'B07WHSJXLF', 'B08C7TYHPB', 'B07WHQWXL7', 'B07WHQBZLS', 'B07WGPKTS4', 'B07WGPKMP5']\n"
     ]
    }
   ],
   "source": [
    "def save_artifacts(model_dir: str, cfg: NBConfig, ue, pe, svd, U, V, content_art, pop_rank):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    artifacts = {\n",
    "        \"config\": asdict(cfg), \"user_encoder\": ue, \"product_encoder\": pe,\n",
    "        \"svd\": svd, \"U\": U, \"V\": V,\n",
    "        \"content_pid2idx\": content_art[\"pid2idx\"], \"content_idx2pid\": content_art[\"idx2pid\"],\n",
    "        \"content_similarity\": content_art[\"sim\"],\n",
    "        \"pop_rank\": pop_rank, \"train_df_cols\": train_df.columns.tolist()\n",
    "    }\n",
    "    dump(artifacts, os.path.join(model_dir, \"hybrid_model.joblib\"))\n",
    "    print(f\"Model artifacts saved to the '{model_dir}' directory.\")\n",
    "\n",
    "save_artifacts(CFG.model_dir, CFG, ue, pe, svd, U, V, content_artifacts, pop_rank)\n",
    "\n",
    "# Example of how to use the saved model for inference\n",
    "def predict_for_user(user_id: str, model_path: str = \"../models/recommendation/hybrid_model.joblib\", top_k: int = 10) -> List[str]:\n",
    "    artifacts = load(model_path)\n",
    "    # Reload components\n",
    "    enc = {\"user\": artifacts[\"user_encoder\"], \"product\": artifacts[\"product_encoder\"]}\n",
    "    U, V = artifacts[\"U\"], artifacts[\"V\"]\n",
    "    pop_rank = artifacts[\"pop_rank\"]\n",
    "    \n",
    "    # Generate recommendations using the collaborative model\n",
    "    recommendations = recommend_collab(user_id, enc, U, V, csr_matrix(U.shape), top_k, pop_rank)\n",
    "    return recommendations\n",
    "\n",
    "# Get a sample user and make a prediction\n",
    "some_user = train_df[\"user_id\"].iloc[10] \n",
    "print(f\"\\n----- Recommendations for User ID: {some_user} -----\")\n",
    "user_recommendations = predict_for_user(some_user, top_k=20)\n",
    "print(user_recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
